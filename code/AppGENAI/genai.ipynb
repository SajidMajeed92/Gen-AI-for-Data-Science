{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACKING_V2\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AppGENAI\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x207e3e341f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/prompt_engineering/concepts\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConcepts | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\\n\\nPrompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nMustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\\nRead the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.\\nStructured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.\\n\\nTags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousPrompt CanvasNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Documents into chunks documents-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='While there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nMustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\\nRead the documentation\\nTools\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='And here is one with mustache:\\nHello, {{name}}!\\nMustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\\nRead the documentation\\nTools\\u200b\\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\\nand JSON schema of arguments used to call the tool.\\nStructured Output\\u200b\\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\\nstick to a specified schema. This may or may not use Tools under the hood.\\nStructured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\\nModel\\u200b'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Model\\u200b\\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\\nPrompt Versioning\\u200b\\nVerisioning is a key part of iterating and collaborating on your different prompts.\\nCommits\\u200b\\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g. prompt_name:commit_hash).\\nIn the UI, you can compare a commit with its previous version by toggling the \"diff\" button in the top-right corner of the Commits tab.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Tags\\u200b\\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\\nPrompt Playground\\u200b\\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\\nIn the playground you can:\\n\\nChange the model being used\\nChange prompt template being used\\nChange the output schema\\nChange the tools available\\nEnter the input variables to run through the prompt template\\nRun the prompt through the model\\nObserve the outputs\\n\\nTesting multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousPrompt CanvasNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=\"AIzaSyDwCCwNfUuyg9ixTt4IxXpGnGRnv3YAjBk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000207E3E9EA70>, model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x207e3f95810>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.\\nWe often see that prompt engineering is multi-disciplinary.\\nSometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\\nor another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"Prompt engineering is important because it allows you to change the way the model behaves.\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000207C3AA6A40>, default_metadata=())\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(model,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Prompt engineering is generally the easiest method to begin modifying a model's behavior and often yields the best return on investment, although other techniques like fine-tuning exist.\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Prompt engineering is important because it allows you to change the way the model behaves.\",\n",
    "    \"context\":[Document(page_content=\" While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x207e3f95810>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000207E3F95810>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000207C3AA6A40>, default_metadata=())\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided text, prompt engineering is one of the core pillars of LangSmith.  It's described as crucial because it allows users to change how a model behaves, often more simply and with higher ROI than other methods like fine-tuning.  A prompt acts like directions to a model, guiding its response without altering its inherent capabilities.  LangSmith provides tools to facilitate prompt engineering, including the ability to test multiple prompts, test over a dataset, store and version prompt templates, and utilize different prompt styles (chat and completion) and formatting (f-string and mustache).\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'LangSmith has two usage limits: total traces and extended',\n",
       " 'context': [Document(id='428a748f-d586-4726-bb9d-b725037a85e1', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.'),\n",
       "  Document(id='91b49b89-e9dd-4de2-bc43-01b092c4de3c', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousPrompt CanvasNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(id='28410347-9ede-436b-8e12-0dd6a941f7b2', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nMustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\\nRead the documentation\\nTools\\u200b'),\n",
       "  Document(id='f177857f-0e70-4605-ad6f-c276736ecb39', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.')],\n",
       " 'answer': \"Based on the provided text, prompt engineering is one of the core pillars of LangSmith.  It's described as crucial because it allows users to change how a model behaves, often more simply and with higher ROI than other methods like fine-tuning.  A prompt acts like directions to a model, guiding its response without altering its inherent capabilities.  LangSmith provides tools to facilitate prompt engineering, including the ability to test multiple prompts, test over a dataset, store and version prompt templates, and utilize different prompt styles (chat and completion) and formatting (f-string and mustache).\"}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='428a748f-d586-4726-bb9d-b725037a85e1', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptUse custom TLS certificatesManage prompts programmaticallyManaging Prompt SettingsPrompt TagsOpen a prompt from a traceTesting over a datasetLangChain HubPrompt CanvasConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringConceptual GuideOn this pageConcepts\\nPrompt engineering is one the core pillars of LangSmith.'),\n",
       " Document(id='91b49b89-e9dd-4de2-bc43-01b092c4de3c', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Testing multiple prompts\\u200b\\nYou can add more prompts to your playground to easily compare outputs and decide which version is better:\\n\\nTesting over a dataset\\u200b\\nTo test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\\nare streamed back as well as how many repitions there are in the test.\\n\\nYou can click on the \"View Experiment\" button to dive deeper into the results of the test.Was this page helpful?You can leave detailed feedback on GitHub.PreviousPrompt CanvasNextDeployment (LangGraph Platform)Why prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(id='28410347-9ede-436b-8e12-0dd6a941f7b2', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!\\nMustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\\nRead the documentation\\nTools\\u200b'),\n",
       " Document(id='f177857f-0e70-4605-ad6f-c276736ecb39', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompt engineering is one the core pillars of LangSmith.\\nWhile traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\\nWe aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\\nWhy prompt engineering?\\u200b\\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\\'s next performance - it guides the model\\'s\\nbehavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\\na prompt provides instructions, examples, and context that shape how the model responds.\\nPrompt engineering is important because it allows you to change the way the model behaves.\\nWhile there are other ways to change the model\\'s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\\nand often provides the highest ROI.')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
